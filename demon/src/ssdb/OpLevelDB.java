package ssdb;import java.io.BufferedReader;import java.io.ByteArrayOutputStream;import java.io.DataOutputStream;import java.io.File;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStreamReader;import java.net.URI;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.atomic.AtomicLong;import net.sf.json.JSONObject;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FileStatus;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.data.Stat;import org.fusesource.leveldbjni.JniDBFactory;import org.iq80.leveldb.DB;import org.iq80.leveldb.DBFactory;import org.iq80.leveldb.Options;public class OpLevelDB {	public static AtomicLong increace_thread_complate_flag = new AtomicLong(0);	public static long start = 0l;	static DB db = null;	static double avg_status_num_max = 500;	public static void main(String[] args) {		System.out.println("程序开始运行.........");		ExecutorService executorService = Executors.newFixedThreadPool(5);		DBFactory factory = JniDBFactory.factory;		Options options = new Options().createIfMissing(true);		String leveldb_data_path = "/home/pubsrv/ssdb-master/var/newData";		try {			db = factory.open(new File(leveldb_data_path), options);		} catch (IOException e1) {			System.out.println(leveldb_data_path + "数据文件打开失败:" + e1);			e1.printStackTrace();		}		start = System.currentTimeMillis();		String uuid = args[0];		System.out.println(uuid + " 数据开始写入 start time:" + start);		System.out.println(read(uuid));		System.out.println("数据导入完毕===============共运行"				+ (System.currentTimeMillis() - start) / 1000 + "秒");		try {			if (db != null) {				db.close();			}			executorService.shutdown();		} catch (IOException e) {			System.out.println(leveldb_data_path + "数据库关闭失败");			e.printStackTrace();		}	}	static String parseDataAttr = "navy_base_data,agent,every_hour_status_count,avg_forward_status,avg_commemts_status,interact_user,avg_status_num";	public static String read(String uid) {		try {			String levelkey = encodeHashCode(uid, "avg_forward_status");			byte[] result = db.get(org.fusesource.leveldbjni.JniDBFactory					.bytes(levelkey));			System.out.println("0"+result.length);			System.out.println(new String(result));			return org.fusesource.leveldbjni.JniDBFactory.asString(result);		} catch (Exception e) {			System.out.println(e.getMessage());			e.printStackTrace();		} finally {			OpLevelDB.increace_thread_complate_flag.incrementAndGet();		}		return null;	}	public static String encodeHashCode(String hashname, String key) {		ByteArrayOutputStream bos = new ByteArrayOutputStream(512);		DataOutputStream dos = new DataOutputStream(bos);		String result = "";		try {			dos.writeBytes("h");			dos.write(hashname.getBytes().length);			dos.writeBytes(hashname);			dos.writeBytes("=");			dos.writeBytes(key);			result = bos.toString();			bos.reset();		} catch (IOException e) {			System.out.println("hash key 编译失败......hashname:" + hashname					+ "key:" + key);			e.printStackTrace();		}		return result;	}}