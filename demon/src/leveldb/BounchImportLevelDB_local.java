package leveldb;import java.io.BufferedReader;import java.io.ByteArrayOutputStream;import java.io.DataOutputStream;import java.io.File;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStreamReader;import java.util.Date;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.atomic.AtomicLong;import net.sf.json.JSONObject;import org.apache.commons.lang.StringUtils;import org.fusesource.leveldbjni.JniDBFactory;import org.iq80.leveldb.DB;import org.iq80.leveldb.DBFactory;import org.iq80.leveldb.Options;import org.iq80.leveldb.WriteBatch;import org.iq80.leveldb.WriteOptions;public class BounchImportLevelDB_local {	public static AtomicLong increace_thread_complate_flag = new AtomicLong(0);	public static long start = 0l;	static DB db = null;	static double avg_status_num_max = 500;	static WriteBatch batch;	static int count = 0;	public static void main(String[] args) {		System.out.println("程序开始运行........."+new Date());		ExecutorService executorService = Executors.newFixedThreadPool(5);		DBFactory factory = JniDBFactory.factory;		Options options = new Options().createIfMissing(true);		String path = "";		String leveldb_data_path = "/home/pubsrv/ssdb-master/var/newDfData";		if (args.length == 1) {			path = args[0];		}		try {			options.writeBufferSize(100000000);			options.blockSize(50*1024*1024);			db = factory.open(new File(leveldb_data_path), options);			batch = db.createWriteBatch();		} catch (IOException e1) {			System.out.println(leveldb_data_path + "数据文件打开失败:" + e1);			e1.printStackTrace();		}		start = System.currentTimeMillis();		System.out.println(path + " 数据开始写入 start time:" + start);		wirte(path);		db.write(batch);		System.out.println("数据导入完毕===============共运行"				+ (System.currentTimeMillis() - start) / 1000 + "秒");		try {			if (db != null) {				db.close();			}			executorService.shutdown();		} catch (IOException e) {			System.out.println(leveldb_data_path + "数据库关闭失败");			e.printStackTrace();		}	}	static String parseDataAttr = "uid,navy_base_data,agent,every_hour_status_count,avg_forward_status,avg_commemts_status,interact_user,avg_status_num";	private static BufferedReader reader;	public static void wirte(String path) {		try {			reader = new BufferedReader(new InputStreamReader(					new FileInputStream(path)));			String line = null;			String[] attrs = StringUtils.splitByWholeSeparator(parseDataAttr,					",");			while ((line = reader.readLine()) != null) {				JsonToLevelDB(attrs, line);			}		} catch (Exception e) {			System.out.println("运行" + "失败" + e);			e.printStackTrace();		} finally {			BounchImportLevelDB_local.increace_thread_complate_flag.incrementAndGet();		}	}	public static void JsonToLevelDB(String[] attrs, String json) {		JSONObject jsonObject = JSONObject.fromObject(json);		String uid = jsonObject.getString(attrs[0]);		// 解析出字段写入 leveldb		for (int i = 1; i < attrs.length; i++) {			// 水度			if ("navy_base_data".equals(attrs[i])) {				String shuidubase = jsonObject.getString("navy_base_data");				String levelkey = encodeHashCode(uid, "shuidu");				batch.put(org.fusesource.leveldbjni.JniDBFactory.bytes(levelkey),						org.fusesource.leveldbjni.JniDBFactory								.bytes(shuidubase));				// 活跃度			} else if ("avg_status_num".equals(attrs[i])) {				double avg_status_num = jsonObject.getDouble("avg_status_num");				double rate = avg_status_num / avg_status_num_max;				double liveness = Math.log((double) (rate + 1)) / Math.log(2);				liveness = Math.round(liveness * 100000) / 100000d;				if (liveness > 1) {					liveness = Double.valueOf(String.valueOf(liveness)							.replaceAll("\\d+\\.[0-9]{1}", "0.9"));				}				String levelkey = encodeHashCode(uid, "liveness");				batch.put(org.fusesource.leveldbjni.JniDBFactory.bytes(levelkey),						org.fusesource.leveldbjni.JniDBFactory.bytes(String								.valueOf(liveness)));			} else {				String value = jsonObject.getString(attrs[i]);				String levelkey = encodeHashCode(uid, attrs[i]);				batch.put(org.fusesource.leveldbjni.JniDBFactory.bytes(levelkey),						org.fusesource.leveldbjni.JniDBFactory.bytes(value));			}						if (count++ % 10000 == 0) {				db.write(batch);				batch = db.createWriteBatch();				System.out.println("insert 10000........."+new Date());			}		}	}	public static String encodeHashCode(String hashname, String key) {		ByteArrayOutputStream bos = new ByteArrayOutputStream(512);		DataOutputStream dos = new DataOutputStream(bos);		String result = "";		try {			dos.writeBytes("h");			dos.write(hashname.getBytes().length);			dos.writeBytes(hashname);			dos.writeBytes("=");			dos.writeBytes(key);			result = bos.toString();			bos.reset();		} catch (IOException e) {			System.out.println("hash key 编译失败......hashname:" + hashname					+ "key:" + key);			e.printStackTrace();		}		return result;	}}